<!DOCTYPE html>
{% extends "base.html" %}
{% block body_block %}
<!-- <style media="screen" type="text/css">

.imagediv{
max-width: 50%;
float:left;
margin: 1em;
margin-bottom: 0em;
margin-left:.5em;
padding-right:8px;
padding-left:0px;
padding-bottom:0px;
}
.trialform{
  display:none;
}
</style> -->


<div style="width:700px;padding-top:50px;min-height:600px;">


  <article class="post">
    <a href="{% url 'ds_blog:pseudospectral_blog1' %}">
      <h3 class="post">Pseudospectral Approximation of Functions</h3>
      <div class="meta_author">
      April 6, 2018  &nbsp; &bull; &nbsp; Joshua Lytle
      </div>
      My graduate research at BYU relied on good boundary value problem (BVP) solvers.
      So I had the chance to review a lot of what was already implemented in Python, and build several of my own implementations.
      One approach that I've really enjoyed learning about is the pseudospectral method.
      This blog post will focus on approximating functions with Chebychev polynomials.
      I plan to eventually discuss a pseudospectral BVP solver.
    </a>
  </article>

  <!-- <hr>

  <article class="post">
    <a href="{% url 'ds_blog:logistic_regression' %}">
      <h3 class="post">Logistic Regression</h3>
      <div class="meta_author">
      March 30, 2018  &nbsp; &bull; &nbsp; Joshua Lytle
      </div>
      Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    </a>
  </article>

  <hr>
  <article class="post">
    <a href="{% url 'ds_blog:linear_regression' %}">
      <h3 class="post">Linear Regression</h3>
      <div class="meta_author">
      March 30, 2018  &nbsp; &bull; &nbsp; Joshua Lytle
      </div>
      Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
    </a>
  </article> -->

  <hr>
  <article class="post">
    <a href="{% url 'ds_blog:evaluation_metrics1' %}">
      <h3 class="post">Evaluation Metrics for Regression Models</h3>
      <div class="meta_author">
      April 6, 2018  &nbsp; &bull; &nbsp; Joshua Lytle
      </div>
      So I recently wanted to clarify my thinking about two commonly used error metrics for evaluating regression models - mean absolute error (MAE) and root mean squared error (RMSE).
      These metrics accomplish similar tasks, but each have their own advantages.
      Let  $\mathcal{E} = (e_1,\ldots,e_n)^T$ be the model errors over the test data $(x_i,y_i)$.

      MAE is a straightforward mean of the absolute value of the errors, $\frac{1}{n}\sum_{i=1}^n |e_i|$.

      RMSE is the square root of the mean of the squared errors, $\sqrt{ \frac{1}{n}\sum_{i=1}^n e_i^2 \,}$.
    </a>
  </article>

  <!-- <hr> -->


</div>



{% endblock %}
